import os
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt

# === –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è ===
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 10
#DATASET_PATH = "D:/Programming/Diploma/datasets/combined_dataset/train"
DATASET_PATH = "D:/Programming/Diploma/NewDatasetDirectory/train_unique_modified"

# === –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω—å ===
datagen = ImageDataGenerator(
    validation_split=0.2,
    rescale=1./255,
    rotation_range=25,
    zoom_range=0.1,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    shear_range=0.2
)

train_gen = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMG_SIZE,
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMG_SIZE,
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    subset='validation',
    shuffle=False
)

# === –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å ===
def build_base_model():
    model = models.Sequential([
        layers.Input(shape=(*IMG_SIZE, 3)),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D(),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(7, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# === –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –º–æ–¥–µ–ª—å ===
def build_improved_model():
    model = models.Sequential([
        layers.Input(shape=(*IMG_SIZE, 3)),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(),

        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(),

        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(),

        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
        layers.Dropout(0.5),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(7, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# === –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª–µ–π ===
base_model = build_base_model()
improved_model = build_improved_model()

# === LR Scheduler —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
lr_scheduler = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# === –ù–∞–≤—á–∞–Ω–Ω—è
print("üöÄ –ù–∞–≤—á–∞–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ...")
history_base = base_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    verbose=1
)

print("\nüöÄ –ù–∞–≤—á–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ...")
history_improved = improved_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[lr_scheduler],
    verbose=1
)

# === –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ–±–æ—Ö –º–æ–¥–µ–ª–µ–π ===
base_model.save("base_emotion_model_modified.keras")
improved_model.save("improved_emotion_model_modified.keras")
print("üíæ –ú–æ–¥–µ–ª—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: base_emotion_model_modified.keras, improved_emotion_model_modified.keras")

# === –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
plt.figure(figsize=(14, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history_base.history['accuracy'], label='Base Train Acc')
plt.plot(history_base.history['val_accuracy'], label='Base Val Acc')
plt.plot(history_improved.history['accuracy'], label='Improved Train Acc')
plt.plot(history_improved.history['val_accuracy'], label='Improved Val Acc')
plt.title("Accuracy Comparison")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
plt.plot(history_base.history['loss'], label='Base Train Loss')
plt.plot(history_base.history['val_loss'], label='Base Val Loss')
plt.plot(history_improved.history['loss'], label='Improved Train Loss')
plt.plot(history_improved.history['val_loss'], label='Improved Val Loss')
plt.title("Loss Comparison")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
